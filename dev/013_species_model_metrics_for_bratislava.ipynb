{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3bd807e",
   "metadata": {},
   "source": [
    "# Evaluation of the first version of the classifier\n",
    "\n",
    "Generate the output on:\n",
    "* GBIF validation dataset\n",
    "* Flemming dataset?\n",
    "\n",
    "Metrics:\n",
    "* Accuracy, precision, recall, F1 --> mini_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "from os.path import exists, join, isdir\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai.vision.all import load_learner, CategoryMap\n",
    "\n",
    "VALID_CONFIG_VERSIONS = [1.0]\n",
    "VALID_IMAGE_EXT = ('.png', '.jpg', '.jpeg', '.tiff', '.tif', '.gif', '.webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Testing of the trained model\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def get_key(scientificName=None, usageKey=None, rank='SPECIES', order='Lepidoptera'):\n",
    "    \"\"\"Returns taxon key from scientific name.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    Requests GBIF API. \n",
    "\n",
    "    If GBIF API returns more than one element, display a warning and return the first element from the list.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://api.gbif.org/v1/species/match?\"\n",
    "\n",
    "    assert usageKey is not None or scientificName is not None, \"One of scientificRank or usageKey must be defined.\"\n",
    "\n",
    "    if usageKey is not None:\n",
    "        url += f\"usageKey={usageKey}&\"\n",
    "    if scientificName is not None:\n",
    "        url += f\"scientificName={scientificName}&\"\n",
    "    if rank is not None:\n",
    "        url += f\"rank={rank}&\"\n",
    "    if order is not None:\n",
    "        url += f\"order={order}\"\n",
    "\n",
    "    x=requests.get(url)\n",
    "    return x.json()\n",
    "\n",
    "def gen_name2id(img_dir):\n",
    "\n",
    "    # list folder names\n",
    "    foldernames = os.listdir(img_dir)\n",
    "\n",
    "    name2id = {\n",
    "        'verbatimScientificName':[],\n",
    "        'familyKey':[],\n",
    "        'genusKey':[],\n",
    "        'speciesKey':[]\n",
    "    }\n",
    "\n",
    "    for i, f in enumerate(foldernames):\n",
    "        k=get_key(scientificName=f)\n",
    "        if k['rank']!='SPECIES':\n",
    "            print(f\"Wrong rank for {f} : {k}\")\n",
    "        if f == 'Tethea or': # Bug fix with GBIF Species API, Tethea or gives the order\n",
    "            k=get_key(usageKey=\"5142971\")\n",
    "        name2id['verbatimScientificName'].append(f)\n",
    "        name2id['familyKey'].append(k['familyKey'])\n",
    "        name2id['genusKey'].append(k['genusKey'])\n",
    "        speciesKey = k['usageKey'] if 'acceptedUsageKey' not in k.keys() else k['acceptedUsageKey']\n",
    "        name2id['speciesKey'].append(speciesKey)\n",
    "    \n",
    "    return name2id\n",
    "\n",
    "def gen_level_idx(vocab, hierarchy):\n",
    "    \"\"\"\n",
    "    Returns a list of integers of the size of vocab indicating the hierarchical level of the taxa at index i.\n",
    "    - Species is level 0, Genus 1, Family 2, etc.\n",
    "    - Missing values are noted with -1.\n",
    "\n",
    "    Args:\n",
    "    - vocab (list): List of taxa names to find levels for.\n",
    "    - hierarchy (pandas.DataFrame): with columns 'speciesKey, genusKey, familyKey'\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of level indices for each taxa in vocab.\n",
    "    \"\"\"\n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['speciesKey', 'genusKey', 'familyKey']\n",
    "    if not all(col in hierarchy.columns for col in required_cols):\n",
    "        raise ValueError(f\"hierarchy DataFrame must contain columns: {required_cols}\")\n",
    "\n",
    "    # Build mapping from key -> level\n",
    "    level_map = {}\n",
    "    for col, level in zip(required_cols, range(len(required_cols))):\n",
    "        # dropna ensures we donâ€™t include NaN values\n",
    "        level_map.update({str(k): level for k in hierarchy[col].dropna().unique()})\n",
    "\n",
    "    # Convert vocab to array and map to levels\n",
    "    levels = [level_map.get(str(taxon), -1) for taxon in vocab]\n",
    "\n",
    "    return np.array(levels)\n",
    "\n",
    "def get_pred_conf(preds:torch.Tensor, vocab:CategoryMap, indices:np.ndarray):\n",
    "    \"\"\"Returns predicted labels and confidence for each pred and for each \n",
    "    hierarchy level.\n",
    "\n",
    "    `preds` is a batch of predictions.\n",
    "    \"\"\"\n",
    "    out_preds = []\n",
    "    out_confs = []\n",
    "    indices = torch.from_numpy(indices)\n",
    "    for i in range(indices.max()+1):\n",
    "        one_level_pred = preds[:,indices==i].cpu().numpy()\n",
    "        one_level_prd = vocab[indices==i][one_level_pred.argmax(axis=1)]\n",
    "        one_level_cnf = one_level_pred.max(axis=1)\n",
    "        out_preds += [one_level_prd]\n",
    "        out_confs += [one_level_cnf]\n",
    "    return np.array(out_preds).swapaxes(0,1), np.array(out_confs).swapaxes(0,1)\n",
    "\n",
    "def save_csv(\n",
    "    fname:str,\n",
    "    filenames:list,\n",
    "    prds:np.ndarray,\n",
    "    cnfs:np.ndarray, \n",
    "    thr:int=0.5,\n",
    "    lbls:np.ndarray=None,\n",
    "    vocab:list|np.ndarray=None,\n",
    "    ):\n",
    "\n",
    "    # Flatten the predictions and confidences\n",
    "    n, p = prds.shape\n",
    "    lvls = np.tile(np.arange(p), n)\n",
    "    prds = prds.flatten(order='C') # C-type \n",
    "    cnfs = cnfs.flatten(order='C')\n",
    "    flns = np.repeat(filenames, p)\n",
    "\n",
    "    df=pd.DataFrame({\n",
    "        'filename':flns,\n",
    "        'level':lvls,\n",
    "        'prediction':prds,\n",
    "        'confidence':cnfs\n",
    "    })\n",
    "    \n",
    "    if lbls is not None:\n",
    "        df[\"label\"] = lbls.flatten(order='C')\n",
    "    if vocab is not None:\n",
    "        df['known_label'] = df['label'].isin(vocab)\n",
    "\n",
    "    # Add instance_id\n",
    "    df['instance_id'] = df.index // 3\n",
    "    df['threshold'] = thr\n",
    "\n",
    "    # Reorganize columns\n",
    "    new_order = [\"instance_id\",\"filename\",\"level\",\"prediction\",\"confidence\",\"threshold\"]\n",
    "    if lbls is not None:\n",
    "        new_order = new_order[:3] + [\"label\"] + new_order[3:]\n",
    "    if vocab is not None:\n",
    "        new_order = new_order + [\"known_label\"]\n",
    "    df = df[new_order]\n",
    "\n",
    "    df.to_csv(fname, index=False)\n",
    "\n",
    "def test(\n",
    "    img_dir:str|Path,\n",
    "    model_path:str|Path,\n",
    "    out_dir:str|Path,\n",
    "    hierarchy_path:str|Path=None,\n",
    "    name2id_path:str|Path=None, # Needed if labels are scientific names instead of GBIF ids.\n",
    "    cpu:bool=False,\n",
    "    ):\n",
    "\n",
    "    if isinstance(model_path, str): model_path = Path(model_path)\n",
    "    if isinstance(img_dir, str): img_dir = Path(img_dir)\n",
    "    if isinstance(out_dir, str): out_dir = Path(out_dir)\n",
    "    if hierarchy_path is not None and isinstance(hierarchy_path, str):\n",
    "        hierarchy_path = Path(hierarchy_path)\n",
    "    if name2id_path is not None and isinstance(name2id_path, str):\n",
    "        name2id_path = Path(name2id_path)\n",
    "    \n",
    "    print(\"Predicting...\")\n",
    "    print(\"Loading model...\")\n",
    "    if model_path.exists():\n",
    "        learn = load_learner(model_path, cpu=cpu)\n",
    "        learn.model = learn.model.eval()\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model not found {model_path}\")\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    print(\"Reading hierarchy...\")\n",
    "    if hierarchy_path is None and hasattr(learn,'hierarchy'):\n",
    "        hierarchy = learn.hierarchy\n",
    "    elif hierarchy_path is None and exists(hierarchy_path):\n",
    "        hierarchy=pd.read_csv(hierarchy_path)\n",
    "    print(\"Hierarchy loaded.\")\n",
    "\n",
    "    # Optionally load name2id\n",
    "    if name2id_path is not None and exists(name2id_path):\n",
    "        print(f\"Found name2id file: {name2id_path}. Loading it...\")\n",
    "        name2id_test = pd.read_csv(name2id_path)\n",
    "    else:\n",
    "        print(\"name2id not specified, trying to generate it...\")\n",
    "        name2id_test = pd.DataFrame(gen_name2id(img_dir=img_dir))\n",
    "        print(\"name2id created.\")\n",
    "\n",
    "    print(\"Reading image filenames...\")\n",
    "    filenames = list(img_dir.rglob(\"*\"))\n",
    "    \n",
    "    filenames = [f for f in filenames if not isdir(f) and f.suffix.lower() in VALID_IMAGE_EXT]\n",
    "    print(f\"Found {len(filenames)} images.\")\n",
    "\n",
    "    # TEST\n",
    "    filenames = filenames[:65]\n",
    "\n",
    "    print(\"Creating test DataLoader...\")\n",
    "    test_dl = learn.dls.test_dl(filenames)\n",
    "    print(\"Test DataLoader created.\")\n",
    "\n",
    "    print(\"Get predictions...\")\n",
    "    preds, _ = learn.get_preds(dl=test_dl)\n",
    "    print(f\"Obtained {len(preds)} predictions.\")\n",
    "\n",
    "    print(\"Format predictions...\")\n",
    "    indices=gen_level_idx(learn.dls.vocab, hierarchy)\n",
    "    prds, cnfs = get_pred_conf(preds, learn.dls.vocab, indices)\n",
    "    print(\"Prediction formatted.\")\n",
    "\n",
    "    if name2id_test is not None:\n",
    "        print(\"Getting labels...\")\n",
    "        name2id_test_dict = {\n",
    "            r['verbatimScientificName']:[\n",
    "                r['speciesKey'],\n",
    "                r['genusKey'],\n",
    "                r['familyKey']]\n",
    "            for _, r in name2id_test.iterrows()}\n",
    "        lbls = np.array([name2id_test_dict[Path(f).parent.name] for f in filenames])\n",
    "    \n",
    "    print(\"Saving CSV...\")\n",
    "    out_path = out_dir / model_path.with_suffix('.csv').name\n",
    "    return dict(\n",
    "        out_path=out_path,\n",
    "        filenames=[Path(f).name for f in filenames],\n",
    "        prds=prds,\n",
    "        cnfs=cnfs,\n",
    "        lbls=lbls,\n",
    "        vocab=learn.dls.vocab\n",
    "        )\n",
    "    save_csv(\n",
    "        out_path,\n",
    "        filenames=[Path(f).name for f in filenames],\n",
    "        prds=prds,\n",
    "        cnfs=cnfs,\n",
    "        lbls=lbls,\n",
    "        vocab=learn.dls.vocab\n",
    "        )\n",
    "    print(f\"CSV saved in {out_path}.\")\n",
    "    print(\"Prediction done.\")\n",
    "\n",
    "def cli(config_path=None):\n",
    "    if config_path is None:\n",
    "        parser = argparse.ArgumentParser(description=\"Main testing script.\")\n",
    "        parser.add_argument(\"-c\", \"--config\", type=str,\n",
    "            help=\"Path to config file.\")\n",
    "        args = parser.parse_args()\n",
    "        config_path = args.config\n",
    "    \n",
    "    if exists(config_path):\n",
    "        # Load config file\n",
    "        with open(config_path) as f:\n",
    "            config=yaml.safe_load(f)\n",
    "\n",
    "        # Check config version\n",
    "        assert float(config['version']) in VALID_CONFIG_VERSIONS, (\n",
    "            f\"Wrong config version: {config['version']}. \"\n",
    "            f\"Must be in {VALID_CONFIG_VERSIONS}.\") \n",
    "        \n",
    "        assert 'test' in config.keys(), (\n",
    "            f\"Wrong config format: {config.keys()} \"\n",
    "            f\"must includes 'test' key and {test.__code__.co_varnames} \"\n",
    "            \"sub-keys.\")\n",
    "\n",
    "        # Start the training\n",
    "        return test(**config['test'])\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Path to config not found: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec102bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=cli(\"/home/george/codes/lepinet/configs/20251104_1_test.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['lbls'].flatten(order=\"C\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490bb653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"/home/george/codes/lepinet/data/lepi/0061420-241126133413365_sampled_processing_metadata_postprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778dc71a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
