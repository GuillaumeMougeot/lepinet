{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937d61a8-8ebe-4e51-9b91-7a999cc0503f",
   "metadata": {},
   "source": [
    "# Why fastai is so slow to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f10a89-3be2-456f-a19c-b198e2d4a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os.path import exists, join\n",
    "import importlib\n",
    "from fastai.vision.all import (\n",
    "    DataBlock,\n",
    "    ImageBlock,\n",
    "    MultiCategoryBlock,\n",
    "    ColSplitter,\n",
    "    ColReader,\n",
    "    Resize,\n",
    "    aug_transforms,\n",
    "    vision_learner,\n",
    "    partial,\n",
    "    F1ScoreMulti,\n",
    "    accuracy_multi,\n",
    "    ShowGraphCallback,\n",
    "    CSVLogger,\n",
    "    EarlyStoppingCallback,\n",
    "    ImageDataLoaders,\n",
    "    SaveModelCallback,\n",
    ")\n",
    "from fastai.callback.tensorboard import TensorBoardCallback\n",
    "# from fastai.distributed import *\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from datetime import datetime \n",
    "import yaml\n",
    "import aiohttp, asyncio\n",
    "from shutil import copyfile\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from fastai.metrics import Metric\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fafaf8-5a8c-41c7-ba19-70eb31ad2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"/home/george/codes/lepinet/configs/20251111_train_ece.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf55e70-412f-46fd-8fce-0d18b63a6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That is 011_lepi_large_prod script\n",
    "VALID_CONFIG_VERSIONS = [1.0]\n",
    "\n",
    "LEVELS = {\n",
    "    \"speciesKey\":\"scientificName\",\n",
    "    \"genusKey\":\"genus\",\n",
    "    \"familyKey\":\"family\",\n",
    "}\n",
    "HIERARCHY_LEVELS = list(LEVELS.keys())\n",
    "\n",
    "def build_hierarchy(df: pd.DataFrame, hierarchy_levels: list):\n",
    "    \"\"\"\n",
    "    Build a hierarchical\n",
    "    \"\"\"\n",
    "    return (\n",
    "    df\n",
    "    .groupby('speciesKey')[hierarchy_levels]\n",
    "    .take([0])\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values(hierarchy_levels[::-1])\n",
    "    )\n",
    "\n",
    "def save_hierarchy(hierarchy: pd.DataFrame, filename: str|Path):\n",
    "    \"\"\"\n",
    "    Save the hierarchy dictionary to a JSON file.\n",
    "    \"\"\"\n",
    "    hierarchy.to_csv(filename, index=False)\n",
    "\n",
    "def load_hierarchy(filename: str|Path):\n",
    "    try:\n",
    "        return pd.read_csv(filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading the hierarchy: {filename}\")\n",
    "\n",
    "def flatten_hierarchy(hierarchy: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Flatten the hierarchy into a sequential list.\n",
    "    \"\"\"\n",
    "    flat_hierarchy = []\n",
    "    for c in hierarchy.columns:\n",
    "        flat_hierarchy.extend(hierarchy[c].unique().astype(str).tolist())\n",
    "    return flat_hierarchy\n",
    "\n",
    "def filter_df(df, remove_in=[], keep_in=[],  img_per_spc=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_per_spc : int, default=0\n",
    "        Number of images per species to select. If 0, then select them all.\n",
    "    \"\"\"\n",
    "    df=df.copy()\n",
    "    # Filter out 'test_ood' rows and 'test_in' rows\n",
    "    if len(remove_in)>0:\n",
    "        df = df[~df['set'].isin(remove_in)]\n",
    "    if len(keep_in)>0:\n",
    "        df = df[df['set'].isin(keep_in)]\n",
    "\n",
    "    # Filter rows if too few images per species\n",
    "    if img_per_spc > 0:\n",
    "        print(f\"Selecting {img_per_spc} images per species.\")\n",
    "        df=df[(df\n",
    "            .groupby('speciesKey')['speciesKey']\n",
    "            .transform('count') > img_per_spc)]\n",
    "    \n",
    "    print(f\"Length of the filtered DataFrame: {len(df)}.\")\n",
    "    return df\n",
    "\n",
    "def prepare_df_v1(df, valid_set='1'):\n",
    "    def generate_image_path(row):\n",
    "        return Path(str(row['speciesKey'])) / row['filename']\n",
    "\n",
    "    # Apply the function to create the image paths\n",
    "    df['image_path'] = df.apply(generate_image_path, axis=1)\n",
    "    # Add a column to specify whether the row is for training or validation\n",
    "    df['is_valid'] = df['set'] == valid_set\n",
    "\n",
    "    # Create a function to extract the labels at different hierarchy levels\n",
    "    def get_hierarchy_labels(row):\n",
    "        return ' '.join(map(str, [row[level] for level in HIERARCHY_LEVELS]))\n",
    "\n",
    "    # Add a column with hierarchy labels\n",
    "    df['hierarchy_labels'] = df.apply(get_hierarchy_labels, axis=1)\n",
    "    # Keep only the columns needed for ImageDataLoaders\n",
    "    df = df[['image_path', 'hierarchy_labels', 'is_valid']]\n",
    "    return df\n",
    "\n",
    "def prepare_df(df, valid_set='1'):\n",
    "    # Vectorized image path creation (no apply)\n",
    "    df = df.copy()\n",
    "    df['image_path'] = df['speciesKey'].astype(str) + '/' + df['filename']\n",
    "\n",
    "    # Vectorized is_valid flag\n",
    "    df['is_valid'] = df['set'].eq(valid_set)\n",
    "\n",
    "    # Vectorized hierarchy label creation\n",
    "    df['hierarchy_labels'] = df[HIERARCHY_LEVELS].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    # Convert image_path to pathlib.Path objects if fastai requires Path type\n",
    "    # df['image_path'] = df['image_path'].map(Path)\n",
    "\n",
    "    return df[['image_path', 'hierarchy_labels', 'is_valid']]\n",
    "\n",
    "class StreamingF1(Metric):\n",
    "    \"Non-accumulating, streaming F1 metric for multi-label tasks.\"\n",
    "    def __init__(self, average='macro', thresh=0.5, sigmoid=True):\n",
    "        self.average, self.thresh, self.sigmoid = average, thresh, sigmoid\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.total, self.count = 0.0, 0\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        preds, targs = learn.pred, learn.y\n",
    "        if self.sigmoid:\n",
    "            preds = torch.sigmoid(preds)\n",
    "        preds = (preds >= self.thresh).float()\n",
    "\n",
    "        # Move to CPU + numpy\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        targs = targs.detach().cpu().numpy()\n",
    "\n",
    "        batch_f1 = f1_score(targs, preds, average=self.average, zero_division=0)\n",
    "        self.total += batch_f1\n",
    "        self.count += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.count == 0: return None\n",
    "        return self.total / self.count\n",
    "    \n",
    "    @property\n",
    "    def name(self):  return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, value): self._name = value\n",
    "\n",
    "class FastStreamingF1(Metric):\n",
    "    \"\"\"\n",
    "    Efficient, streaming F1 metric for multi-label problems.\n",
    "    Keeps only running TP/FP/FN counts.\n",
    "    \"\"\"\n",
    "    def __init__(self, average='macro', thresh=0.5, sigmoid=True, name=None):\n",
    "        self.average, self.thresh, self.sigmoid = average, thresh, sigmoid\n",
    "        self.name = name or f\"F1({average})\"\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.tp = self.fp = self.fn = None\n",
    "        self.count = 0  # for 'macro' averaging\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        preds, targs = learn.pred, learn.y\n",
    "        if self.sigmoid: preds = torch.sigmoid(preds)\n",
    "        preds = (preds >= self.thresh).float()\n",
    "\n",
    "        # Compute true positives, false positives, false negatives per class\n",
    "        tp = (preds * targs).sum(dim=0)\n",
    "        fp = (preds * (1 - targs)).sum(dim=0)\n",
    "        fn = ((1 - preds) * targs).sum(dim=0)\n",
    "\n",
    "        if self.tp is None:\n",
    "            self.tp, self.fp, self.fn = tp, fp, fn\n",
    "        else:\n",
    "            self.tp += tp\n",
    "            self.fp += fp\n",
    "            self.fn += fn\n",
    "        self.count += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        # Compute per-class precision, recall, f1\n",
    "        precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "        recall = self.tp / (self.tp + self.fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "        if self.average == 'macro':\n",
    "            return f1.mean().item()\n",
    "        elif self.average == 'micro':\n",
    "            # Micro average: sum over all classes\n",
    "            tp, fp, fn = self.tp.sum(), self.fp.sum(), self.fn.sum()\n",
    "            precision = tp / (tp + fp + 1e-8)\n",
    "            recall = tp / (tp + fn + 1e-8)\n",
    "            return (2 * precision * recall / (precision + recall + 1e-8)).item()\n",
    "        elif self.average == 'samples':\n",
    "            # optional extension â€” per-sample averaging\n",
    "            # could be added easily if you need it later\n",
    "            return f1.mean().item()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported average: {self.average}\")\n",
    "        \n",
    "    @property\n",
    "    def name(self):  return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, value): self._name = value\n",
    "\n",
    "async def get_key(session, scientificName=None, usageKey=None, rank='SPECIES', order='Lepidoptera'):\n",
    "    url = \"https://api.gbif.org/v1/species/match?\"\n",
    "    assert usageKey is not None or scientificName is not None, \"One of scientificName or usageKey must be defined.\"\n",
    "\n",
    "    if usageKey is not None:\n",
    "        url += f\"usageKey={usageKey}&\"\n",
    "    if scientificName is not None:\n",
    "        url += f\"scientificName={scientificName}&\"\n",
    "    if rank is not None:\n",
    "        url += f\"rank={rank}&\"\n",
    "    if order is not None:\n",
    "        url += f\"order={order}\"\n",
    "\n",
    "    async with session.get(url) as response:\n",
    "        r = await response.json()\n",
    "        return r['canonicalName']\n",
    "\n",
    "async def get_all_keys(vocab):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [get_key(session, usageKey=k, rank=None) for k in vocab]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "def gen_dls(\n",
    "    parquet_path: str|Path,\n",
    "    img_dir: str|Path,\n",
    "    out_dir: str|Path,\n",
    "    img_per_spc: int,\n",
    "    fold: str,\n",
    "    model_name: str,\n",
    "    nb_epochs: int,\n",
    "    batch_size: int,\n",
    "    aug_img_size: int,\n",
    "    img_size: int,\n",
    "    model_arch_name: str,\n",
    "    hierarchy_path: str|Path = None,\n",
    "    ):\n",
    "    # Assert types\n",
    "    if isinstance(parquet_path, str): parquet_path = Path(parquet_path)\n",
    "    if isinstance(img_dir, str): img_dir = Path(img_dir)\n",
    "    if isinstance(out_dir, str): out_dir = Path(out_dir)\n",
    "    if hierarchy_path is None:\n",
    "        hierarchy_path = parquet_path.parent / \"hierarchy.csv\"\n",
    "    \n",
    "    # First check if an existing preprocessed df exists, and if so, load it\n",
    "    parquet_name = Path(parquet_path.name)\n",
    "    df_path = out_dir.parent / parquet_name.with_suffix(\".lepinet.parquet\")\n",
    "    if df_path.exists() and hierarchy_path.exists():\n",
    "        print(f\"Found existing preprocessed df: {df_path}\")\n",
    "        print(\"Loading it...\")\n",
    "        df = pd.read_parquet(df_path)\n",
    "\n",
    "        hierarchy=load_hierarchy(filename=hierarchy_path)\n",
    "        vocab=flatten_hierarchy(hierarchy)\n",
    " \n",
    "        print(\"Df and vocab loaded.\")\n",
    "    # Else, preprocessed the DataFrame\n",
    "    elif parquet_path.exists():\n",
    "        print(f\"Loading parquet file {parquet_path}\")\n",
    "        # Read parquet \n",
    "        df=pd.read_parquet(parquet_path)\n",
    "\n",
    "        # Filter rows\n",
    "        print(\"Filtering rows...\")\n",
    "        df=filter_df(df, remove_in=[\"0\"], img_per_spc=img_per_spc)\n",
    "        print(\"DataFrame filtered.\")\n",
    "\n",
    "        # Read or create hierarchy path\n",
    "        if not hierarchy_path.exists():\n",
    "            print(f\"Hierarchy not found in {hierarchy_path}. Creating it...\")\n",
    "            hierarchy=build_hierarchy(df, hierarchy_levels = HIERARCHY_LEVELS)\n",
    "            save_hierarchy(hierarchy, filename=hierarchy_path)\n",
    "            print(f\"Hierarchy saved in {hierarchy_path}.\")\n",
    "        \n",
    "        # Read hierarchy file\n",
    "        hierarchy=load_hierarchy(filename=hierarchy_path)\n",
    "        vocab=flatten_hierarchy(hierarchy)\n",
    "\n",
    "        # Remove test_ood and test_in data\n",
    "        print(\"Preparing DataFrame...\")\n",
    "        df = prepare_df(df, valid_set=fold)\n",
    "        print(\"DataFrame ready.\")\n",
    "\n",
    "        # Save the preprocessed DataFrame for later use\n",
    "        print(f\"Saving the DataFrame to {df_path}\")\n",
    "        df.to_parquet(df_path, index=False)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Parquet path not found: {parquet_path}\")\n",
    "\n",
    "    datablock = DataBlock(\n",
    "        blocks=(ImageBlock, MultiCategoryBlock(vocab=vocab)),\n",
    "        splitter=ColSplitter(),\n",
    "        get_x=ColReader(0, pref=img_dir),\n",
    "        get_y=ColReader(1, label_delim=' '),\n",
    "        item_tfms=Resize(aug_img_size),\n",
    "        batch_tfms=aug_transforms(size=img_size)\n",
    "    )\n",
    "    dls = datablock.dataloaders(df, bs=batch_size)\n",
    "    # dls.train.num_workers = 16\n",
    "    # dls.valid.num_workers = 8\n",
    "    print(f\"Number of workers: {dls.num_workers}, {dls.train.num_workers}, {dls.valid.num_workers}\")\n",
    "\n",
    "    return dls, hierarchy\n",
    "\n",
    "def train(\n",
    "    parquet_path: str|Path,\n",
    "    img_dir: str|Path,\n",
    "    out_dir: str|Path,\n",
    "    img_per_spc: int,\n",
    "    fold: str,\n",
    "    model_name: str,\n",
    "    nb_epochs: int,\n",
    "    batch_size: int,\n",
    "    aug_img_size: int,\n",
    "    img_size: int,\n",
    "    model_arch_name: str,\n",
    "    hierarchy_path: str|Path = None,\n",
    "    ):\n",
    "    # Assert types\n",
    "    if isinstance(parquet_path, str): parquet_path = Path(parquet_path)\n",
    "    if isinstance(img_dir, str): img_dir = Path(img_dir)\n",
    "    if isinstance(out_dir, str): out_dir = Path(out_dir)\n",
    "    if hierarchy_path is None:\n",
    "        hierarchy_path = parquet_path.parent / \"hierarchy.csv\"\n",
    "        \n",
    "    dls, hierarchy = gen_dls(\n",
    "        parquet_path=parquet_path,\n",
    "        img_dir=img_dir,\n",
    "        out_dir=out_dir,\n",
    "        img_per_spc=img_per_spc,\n",
    "        fold=fold,\n",
    "        model_name=model_name,\n",
    "        nb_epochs=nb_epochs,\n",
    "        batch_size=batch_size,\n",
    "        aug_img_size=aug_img_size,\n",
    "        img_size=img_size,\n",
    "        model_arch_name=model_arch_name,\n",
    "        hierarchy_path=hierarchy_path,\n",
    "    )\n",
    "\n",
    "    # TODO: To use or to remove:\n",
    "    # dls = ImageDataLoaders.from_df(\n",
    "    #     df,\n",
    "    #     img_dir,\n",
    "    #     valid_col='is_valid',\n",
    "    #     label_delim=' ',\n",
    "    #     bs=batch_size,\n",
    "    #     item_tfms=Resize(aug_img_size),\n",
    "    #     batch_tfms=aug_transforms(size=img_size))\n",
    "\n",
    "    # f1_macro = F1ScoreMulti(thresh=0.5, average='macro')\n",
    "    # f1_macro.name = 'F1(macro)'\n",
    "    # f1_micro = F1ScoreMulti(thresh=0.5, average='micro')\n",
    "    # f1_micro.name = 'F1(micro)'\n",
    "    f1_macro = FastStreamingF1(average='macro', thresh=0.5)\n",
    "    f1_macro.name = 'F1(macro)'\n",
    "    f1_micro = FastStreamingF1(average='micro', thresh=0.5)\n",
    "    f1_micro.name = 'F1(micro)'\n",
    "\n",
    "    model_arch = getattr(importlib.import_module('fastai.vision.all'), model_arch_name)\n",
    "\n",
    "    learn = vision_learner(\n",
    "        dls, \n",
    "        model_arch, \n",
    "        metrics=[partial(accuracy_multi, thresh=0.5), f1_macro, f1_micro],\n",
    "        model_dir=out_dir / \"models\",\n",
    "        cbs=[\n",
    "            CSVLogger(out_dir/f\"{model_name}.csv\", append=True),\n",
    "            # TensorBoard logging\n",
    "            TensorBoardCallback(\n",
    "                log_dir=out_dir/'tensorboard',  # where to store logs\n",
    "                trace_model=False,              # disable tracing to save memory\n",
    "                log_preds=False,                # optional: skip predictions logging\n",
    "            ),\n",
    "            \n",
    "            # Automatically save best model and optionally every epoch\n",
    "            SaveModelCallback(\n",
    "                fname=f\"{model_name}\",\n",
    "                every_epoch=True\n",
    "            ),\n",
    "\n",
    "            # EarlyStoppingCallback(patience=10),\n",
    "            ])\n",
    "\n",
    "    \n",
    "    # with learn.distrib_ctx():\n",
    "    learn.fine_tune(nb_epochs, 2e-2, freeze_epochs=0)\n",
    "\n",
    "    # --- Debug mode: run validation only ---\n",
    "    # Run validation directly to test metrics and memory\n",
    "    # val_loss, val_metrics = learn.validate()\n",
    "    # print(f\"Validation results:\\nLoss: {val_loss}\\nMetrics: {val_metrics}\")\n",
    "    # return\n",
    "\n",
    "    # Save the model\n",
    "    # ... remove cbs first\n",
    "    # learn.recorder = None\n",
    "    # learn.remove_cbs((CSVLogger,EarlyStoppingCallback))\n",
    "    # import gc; gc.collect()\n",
    "\n",
    "    # ...recreate a vision learner to remove large files that lives inside learner\n",
    "    slim_learn = vision_learner(learn.dls, model_arch)\n",
    "    slim_learn.model = learn.model\n",
    "\n",
    "    # Integrate hierarchy\n",
    "    slim_learn.hierarchy = hierarchy\n",
    "\n",
    "    # Integrate id2name\n",
    "    id2name = asyncio.run(get_all_keys(slim_learn.dls.vocab))\n",
    "    id2name = {v:n for (v,n) in zip(slim_learn.dls.vocab, id2name)}\n",
    "    slim_learn.id2name=id2name\n",
    "\n",
    "    model_path = out_dir / f\"{model_name}.pkl\"\n",
    "    slim_learn.export(model_path)\n",
    "\n",
    "def create_out_dir(out_dir, desc):\n",
    "    \"\"\"Create the output directory named after datetime-desc\"\"\"\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    out_dirname = join(out_dir, current_time + '-' + desc)\n",
    "    os.makedirs(out_dirname, exist_ok=True)\n",
    "    return out_dirname\n",
    "\n",
    "def cli(config_path:str|Path=None):\n",
    "    if config_path is None:\n",
    "        parser = argparse.ArgumentParser(description=\"Main training file.\")\n",
    "        parser.add_argument(\"-c\", \"--config\", type=str,\n",
    "            help=\"Path to config file.\")\n",
    "        args = parser.parse_args()\n",
    "        config_path = args.config\n",
    "    \n",
    "    if exists(config_path):\n",
    "        # Load config file\n",
    "        with open(config_path) as f:\n",
    "            config=yaml.safe_load(f)\n",
    "\n",
    "        # Check config version\n",
    "        assert float(config['version']) in VALID_CONFIG_VERSIONS, (\n",
    "            f\"Wrong config version: {config['version']}. \"\n",
    "            f\"Must be in {VALID_CONFIG_VERSIONS}.\") \n",
    "        \n",
    "        # Create and edit the output directory\n",
    "        config['train']['out_dir'] = create_out_dir(\n",
    "            config['train']['out_dir'], config['desc'])\n",
    "        \n",
    "        # Put the config file inside the output dir\n",
    "        copyfile(config_path, join(config['train']['out_dir'], 'config.yaml'))\n",
    "\n",
    "        # TODO: if a 'test' key exists in the config dict, then modify the \n",
    "        # value of 'model_path' key to be set as the above folder.\n",
    "        \n",
    "        # Start the training\n",
    "        train(**config['train'])\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Path to config not found: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fdc6b-c1a1-471e-bedf-e170804e6ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cli(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57e458-a8a5-44b8-bd59-4a35aebb650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path) as f:\n",
    "    config=yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aeb859-858c-40a8-9bab-b6484c6ee7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls, hierarchy = gen_dls(**config['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf533117-9604-4abf-81ab-4ad8033dfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(config['train']['out_dir'])\n",
    "model_name = config['train']['model_name']\n",
    "model_arch_name = config['train']['model_arch_name']\n",
    "\n",
    "f1_macro = FastStreamingF1(average='macro', thresh=0.5)\n",
    "f1_macro.name = 'F1(macro)'\n",
    "f1_micro = FastStreamingF1(average='micro', thresh=0.5)\n",
    "f1_micro.name = 'F1(micro)'\n",
    "\n",
    "model_arch = getattr(importlib.import_module('fastai.vision.all'), model_arch_name)\n",
    "\n",
    "learn = vision_learner(\n",
    "    dls, \n",
    "    model_arch, \n",
    "    metrics=[partial(accuracy_multi, thresh=0.5), f1_macro, f1_micro],\n",
    "    model_dir= out_dir / \"models\",\n",
    "    cbs=[\n",
    "        CSVLogger(out_dir/f\"{model_name}.csv\", append=True),\n",
    "        # Automatically save best model and optionally every epoch\n",
    "        SaveModelCallback(\n",
    "            fname=f\"{model_name}\"\n",
    "        ),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4d64c-061a-4852-a45b-32f955b086d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595cfd2-c74f-4d5a-b680-d1ec99edbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5, lr_max=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c5191-1f5f-4ff4-a810-a7f365015779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
