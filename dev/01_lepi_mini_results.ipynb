{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "from fastai.vision.all import *\n",
    "from pprint import pprint\n",
    "from inspect import getmembers, getmembers_static\n",
    "from sklearn.metrics import f1_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path=Path(\"/home/george/codes/lepinet/data/mini/0013397-241007104925546_processing_metadata_postprocessed.parquet\")\n",
    "images_path=Path(\"/home/george/codes/lepinet/data/mini/images\")\n",
    "root_path=Path(\"/home/george/codes/lepinet/data/mini\")\n",
    "export_path=Path(\"/home/george/codes/lepinet/data/mini/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df=pd.read_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model_path = export_path / \"00_lepi_mini_model1\"\n",
    "\n",
    "learn = load_learner(model_path)\n",
    "learn.model.eval().to(\"cuda\")\n",
    "len(learn.dls.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hierarchy\n",
    "with open(root_path/\"hierarchy_train.json\", \"r\") as file:\n",
    "    hierarchy=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df, remove_in=[], keep_in=[]):\n",
    "    # Filter out 'test_ood' rows and 'test_in' rows\n",
    "    if len(remove_in)>0:\n",
    "        df = df[~df['set'].isin(remove_in)]\n",
    "    if len(keep_in)>0:\n",
    "        df = df[df['set'].isin(keep_in)]\n",
    "    def generate_image_path(row):\n",
    "        return Path(str(row['speciesKey'])) / row['filename']\n",
    "\n",
    "    # Apply the function to create the image paths\n",
    "    df['image_path'] = df.apply(generate_image_path, axis=1)\n",
    "    # Add a column to specify whether the row is for training or validation\n",
    "    df['is_valid'] = df['set'] == '0'\n",
    "    # Define the hierarchical levels\n",
    "    hierarchy_levels = [\"familyKey\", \"genusKey\", \"speciesKey\"]\n",
    "\n",
    "    # Create a function to extract the labels at different hierarchy levels\n",
    "    def get_hierarchy_labels(row):\n",
    "        return ' '.join(map(str, [row[level] for level in hierarchy_levels]))\n",
    "\n",
    "    # Add a column with hierarchy labels\n",
    "    df['hierarchy_labels'] = df.apply(get_hierarchy_labels, axis=1)\n",
    "    # Keep only the columns needed for ImageDataLoaders\n",
    "    df = df[['image_path', 'hierarchy_labels', 'is_valid']]\n",
    "    return df\n",
    "\n",
    "df_val = prepare_df(pd.read_parquet(parquet_path), keep_in=[\"0\"])\n",
    "df_train = prepare_df(pd.read_parquet(parquet_path), remove_in=[\"test_ood\"])\n",
    "df_ood = prepare_df(pd.read_parquet(parquet_path), keep_in=[\"test_ood\"])\n",
    "df_all = prepare_df(pd.read_parquet(parquet_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on one image\n",
    "pred=learn.predict(images_path/df_val[\"image_path\"].iloc[0])\n",
    "pred_classes, pred_one_hot, pred_proba = pred\n",
    "pred_classes, type(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targs=df_val[\"hierarchy_labels\"].iloc[0].split(\" \")\n",
    "targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = F1ScoreMulti(thresh=0.5, average='macro')\n",
    "f1_macro(pred_classes, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_df(\n",
    "    df_train,\n",
    "    images_path,\n",
    "    valid_col='is_valid',\n",
    "    label_delim=' ',\n",
    "    item_tfms=Resize(460),\n",
    "    batch_tfms=aug_transforms(size=224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getmembers_static(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getmembers_static(dls.loaders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dls.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that one_batch give the same batch and is not randomly generated.\n",
    "for i in range(2):\n",
    "    batch=dls.valid.one_batch()\n",
    "    pprint(batch[0][0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_in, batch_targs = batch\n",
    "batch_in.shape, batch_targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate(dl=dls.valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* F1 macro on the validation set\n",
    "* F1 macro on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(np.array([[0,1]]), np.array([[0,1]]), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Out-of-distribution species\n",
    "\n",
    "The evaluation function must be able to deal with two different vocab, one \n",
    "for the testing set and one for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_vocab(df):\n",
    "    vocab=[]\n",
    "    for i, row in df.iterrows():\n",
    "        vocab += row[\"hierarchy_labels\"].split()\n",
    "    vocab = sorted(np.unique(vocab).tolist())\n",
    "    return vocab\n",
    "\n",
    "test_eq(define_vocab(df_train), learn.dls.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(define_vocab(df_val)), len(define_vocab(df_train)), len(define_vocab(df_ood)), len(define_vocab(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class OODCallback(Callback):b\n",
    "#     run_valid = True\n",
    "#     def after_pred(self,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(images_path/df_all[\"image_path\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_ood = df_ood[\"image_path\"].apply(lambda x: str(images_path/x)).tolist()\n",
    "targs_ood = df_ood[\"hierarchy_labels\"]\n",
    "\n",
    "test_dl=learn.dls.test_dl(filenames_ood)\n",
    "\n",
    "preds, _ = learn.get_preds(dl=test_dl)\n",
    "len(preds), len(filenames_ood), (preds>.5).sum(1), len(targs_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some predictions alongside their ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(preds):\n",
    "    classes = learn.dls.vocab[p>.5]\n",
    "    if len(classes) > 0:\n",
    "        print(i, classes, targs_ood.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_taxon_imgs(\n",
    "        taxon_id: str, \n",
    "        images_dir: str, \n",
    "        hierarchy: dict, \n",
    "        n: int = 5,\n",
    "        image_formats: tuple = (\n",
    "            '.jpg', '.jpeg', '.png', '.gif', '.tiff', '.tif', '.webp')\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Given a taxon ID and a hierarchy, select a random set of n images located in images_dir.\n",
    "    If taxon_id is a species, images are selected from its folder.\n",
    "    If taxon_id is a higher-level category, images are selected from all associated species' folders.\n",
    "    \n",
    "    Returns a list of image file paths.\n",
    "    \"\"\"\n",
    "    species_folders = set()\n",
    "\n",
    "    def find_species(node, path=[]):\n",
    "        \"\"\"Recursively collect all species belonging to a taxon.\"\"\"\n",
    "        for key, subnode in node.items():\n",
    "            if isinstance(subnode, list):  # This is the penultimate level\n",
    "                if taxon_id == key or taxon_id in path:  \n",
    "                    species_folders.update(subnode)\n",
    "                elif taxon_id in subnode: # If taxon is a species\n",
    "                    species_folders.add(taxon_id)\n",
    "            elif isinstance(subnode, dict):  # Higher level, continue traversal\n",
    "                find_species(subnode, path + [key])\n",
    "\n",
    "    # Identify relevant species folders\n",
    "    find_species(hierarchy)\n",
    "\n",
    "    # Collect all image file paths\n",
    "    image_paths = []\n",
    "    for species in species_folders:\n",
    "        species_path = os.path.join(images_dir, species)\n",
    "        if os.path.isdir(species_path):\n",
    "            image_files = [os.path.join(species_path, f) for f in os.listdir(species_path) if f.lower().endswith(image_formats)]\n",
    "            image_paths.extend(image_files)\n",
    "\n",
    "    # Select a random subset of images\n",
    "    return random.sample(image_paths, min(n, len(image_paths)))\n",
    "\n",
    "def show_files(filenames, suptitle=None):\n",
    "    # Load images\n",
    "    ims = [Image.open(f) for f in filenames]\n",
    "    titles=[Path(f).parent.name for f in filenames]\n",
    "    show_images(ims, titles=titles, suptitle=suptitle)\n",
    "\n",
    "def show_lepi(taxon_id):\n",
    "    \"\"\"Display samples for the given taxon id.\n",
    "    \"\"\"\n",
    "    filenames=select_taxon_imgs(\n",
    "        taxon_id=taxon_id,\n",
    "        images_dir=images_path,\n",
    "        hierarchy=hierarchy,\n",
    "    )\n",
    "    show_files(filenames, suptitle=taxon_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "family_images=select_taxon_imgs(\n",
    "    taxon_id=\"7015\",\n",
    "    images_dir=images_path,\n",
    "    hierarchy=hierarchy,\n",
    ")\n",
    "genus_images=select_taxon_imgs(\n",
    "    taxon_id=\"1768691\",\n",
    "    images_dir=images_path,\n",
    "    hierarchy=hierarchy,\n",
    ")\n",
    "species_images=select_taxon_imgs(\n",
    "    taxon_id=\"1768749\",\n",
    "    images_dir=images_path,\n",
    "    hierarchy=hierarchy,\n",
    ")\n",
    "family_images, genus_images, species_images\n",
    "\n",
    "show_files(family_images, suptitle=\"Family\")\n",
    "show_files(genus_images, suptitle=\"Genus\")\n",
    "show_files(species_images, suptitle=\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the model outputs into hierarchy levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_level_idx(vocab, hierarchy):\n",
    "    \"\"\"\n",
    "    Returns a list of integers of the size of vocab indicating the hierarchical level of the taxa at index i.\n",
    "    - Species is level 0, Genus 1, Family 2, etc.\n",
    "    - Missing values are noted with -1.\n",
    "\n",
    "    Args:\n",
    "    - vocab (list): List of taxa names to find levels for.\n",
    "    - hierarchy (dict): Nested dictionary representing taxonomic hierarchy.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of level indices for each taxa in vocab.\n",
    "    \"\"\"\n",
    "    level_lookup = {}\n",
    "\n",
    "    def traverse(node, level=0):\n",
    "        \"\"\"Recursively traverse the hierarchy and store levels.\"\"\"\n",
    "        for key, subnode in node.items():\n",
    "            level_lookup[key] = level  # Assign level to the taxon\n",
    "            if isinstance(subnode, dict):\n",
    "                traverse(subnode, level + 1)\n",
    "            elif isinstance(subnode, list):  # Leaf nodes (species level)\n",
    "                for species in subnode:\n",
    "                    level_lookup[species] = level + 1\n",
    "\n",
    "    # Build the level lookup dictionary\n",
    "    traverse(hierarchy)  # Start from -1 so species end up at level 0\n",
    "\n",
    "    # Assign levels to vocab, default to -1 if missing\n",
    "    indices = np.array([level_lookup.get(v, -1) for v in vocab], dtype=int)\n",
    "\n",
    "    # Invert the indices, so species is 0, genus is 1 etc\n",
    "    indices = np.where(indices < 0, indices, indices.max()-indices)\n",
    "\n",
    "    # Warning for missing values\n",
    "    missing_count = np.sum(indices == -1)\n",
    "    if missing_count > 0:\n",
    "        print(f\"[Warning] Missing values in taxa dictionary: {missing_count}.\")\n",
    "\n",
    "    return indices\n",
    "\n",
    "def split_pred(pred:torch.Tensor, indices:np.ndarray):\n",
    "    out_pred = []\n",
    "    for i in range(indices.max()+1):\n",
    "        out_pred += [pred[indices==i].cpu().numpy()>.5]\n",
    "    return out_pred\n",
    "\n",
    "indices=gen_level_idx(learn.dls.vocab, hierarchy)\n",
    "print(len(indices))\n",
    "one_pred=split_pred(preds[0], indices)\n",
    "for i in range(len(one_pred)):\n",
    "    print(len(one_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
