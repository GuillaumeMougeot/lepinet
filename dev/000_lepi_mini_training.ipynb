{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from fastai.vision.all import *\n",
    "from PIL import Image\n",
    "import asyncio\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path=Path(\"/home/george/codes/lepinet/data/mini/0013397-241007104925546_processing_metadata_postprocessed.parquet\")\n",
    "big_parquet_path=Path(\"/home/george/codes/lepinet/data/lepi/0061420-241126133413365_sampled_processing_metadata_postprocessed.parquet\")\n",
    "images_path=Path(\"/home/george/codes/lepinet/data/mini/images\")\n",
    "root_path=Path(\"/home/george/codes/lepinet/data/mini\")\n",
    "export_path=Path(\"/home/george/codes/lepinet/data/mini/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df, remove_in=[], keep_in=[]):\n",
    "    # Filter out 'test_ood' rows and 'test_in' rows\n",
    "    if len(remove_in)>0:\n",
    "        df = df[~df['set'].isin(remove_in)]\n",
    "    if len(keep_in)>0:\n",
    "        df = df[df['set'].isin(keep_in)]\n",
    "    def generate_image_path(row):\n",
    "        return Path(str(row['speciesKey'])) / row['filename']\n",
    "\n",
    "    # Apply the function to create the image paths\n",
    "    df['image_path'] = df.apply(generate_image_path, axis=1)\n",
    "    # Add a column to specify whether the row is for training or validation\n",
    "    df['is_valid'] = df['set'] == '0'\n",
    "    # Define the hierarchical levels\n",
    "    hierarchy_levels = [\"familyKey\", \"genusKey\", \"speciesKey\"]\n",
    "\n",
    "    # Create a function to extract the labels at different hierarchy levels\n",
    "    def get_hierarchy_labels(row):\n",
    "        return ' '.join(map(str, [row[level] for level in hierarchy_levels]))\n",
    "\n",
    "    # Add a column with hierarchy labels\n",
    "    df['hierarchy_labels'] = df.apply(get_hierarchy_labels, axis=1)\n",
    "    # Keep only the columns needed for ImageDataLoaders\n",
    "    df = df[['image_path', 'hierarchy_labels', 'is_valid']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[0]\n",
    "row.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = images_path / row[\"speciesKey\"] / row[\"filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path, os.path.isfile(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(df['set'].isin(['test_ood', '0'])), \n",
    "sum(df['set'].isin(['test_ood'])),\n",
    "sum(df['set'].isin(['0']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=prepare_df(df.copy(), remove_in=['test_ood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_df(\n",
    "    df,\n",
    "    images_path,\n",
    "    valid_col='is_valid',\n",
    "    label_delim=' ',\n",
    "    item_tfms=Resize(460),\n",
    "    batch_tfms=aug_transforms(size=224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = F1ScoreMulti(thresh=0.5, average='macro')\n",
    "f1_macro.name = 'F1(macro)'\n",
    "f1_samples = F1ScoreMulti(thresh=0.5, average='samples')\n",
    "f1_samples.name = 'F1(samples)'\n",
    "learn = vision_learner(\n",
    "    dls, \n",
    "    resnet50, \n",
    "    metrics=[partial(accuracy_multi, thresh=0.5), f1_macro, f1_samples],\n",
    "    cbs=[ShowGraphCallback(), CSVLogger(export_path/\"history1.csv\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(10, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "model_path = export_path / \"00_lepi_mini_model1\"\n",
    "learn.export(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh /home/george/codes/lepinet/data/mini/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = export_path / \"00_lepi_mini_model1\"\n",
    "\n",
    "learn = load_learner(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls.vocab[:10], len(learn.dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model version\n",
    "\n",
    "I need to specify the vocab of the MultiCategoryBlock\n",
    "\n",
    "So I need to go down in the layered architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_vocab_v1(df):\n",
    "    vocab=[]\n",
    "    hierarchy_levels = [\"familyKey\", \"genusKey\", \"speciesKey\"]\n",
    "    for col in hierarchy_levels:\n",
    "        vocab += pd.unique(df[col]).tolist()\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab \n",
    "\n",
    "def build_hierarchy_v1(df: pd.DataFrame, hierarchy_levels: list):\n",
    "    \"\"\"\n",
    "    Build a hierarchical tree where the penultimate level holds a unique list of the lowest-level values.\n",
    "    \"\"\"\n",
    "    hierarchy = defaultdict(lambda: defaultdict(set))  # Use set to avoid duplicates\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        current_level = hierarchy\n",
    "        for i, level in enumerate(hierarchy_levels):\n",
    "            key = row[level]\n",
    "\n",
    "            if i == len(hierarchy_levels) - 2:  # Penultimate level\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = set()\n",
    "                current_level = current_level[key]\n",
    "            elif i == len(hierarchy_levels) - 1:  # Lowest level (store unique values)\n",
    "                current_level.add(key)\n",
    "            else:\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = defaultdict(set)\n",
    "                current_level = current_level[key] # Goes deeper in the hierarchy\n",
    "\n",
    "    # Convert sets to lists for the final output\n",
    "    def convert_sets_to_lists(node):\n",
    "        if isinstance(node, dict):\n",
    "            return {k: convert_sets_to_lists(v) for k, v in node.items()}\n",
    "        elif isinstance(node, set):\n",
    "            return list(node)\n",
    "        return node\n",
    "\n",
    "    return convert_sets_to_lists(hierarchy)\n",
    "\n",
    "def build_hierarchy(df: pd.DataFrame, hierarchy_levels: list):\n",
    "    \"\"\"\n",
    "    Build a hierarchical tree where the penultimate level holds a unique list of the lowest-level values.\n",
    "    \"\"\"\n",
    "    hierarchy = defaultdict(lambda: defaultdict(set))  # Use set to avoid duplicates\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        current_level = hierarchy\n",
    "        for level in hierarchy_levels:\n",
    "            key = row[level]\n",
    "\n",
    "            if level == \"speciesKey\":  # Lowest level (store unique values)\n",
    "                current_level.add(key)\n",
    "            else:\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = set() if level == \"genusKey\" else defaultdict(set)  # Penultimate level\n",
    "                current_level = current_level[key] # Goes deeper in the hierarchy\n",
    "\n",
    "    # Convert sets to lists for the final output\n",
    "    def convert_sets_to_lists(node):\n",
    "        if isinstance(node, dict):\n",
    "            return {k: convert_sets_to_lists(v) for k, v in node.items()}\n",
    "        elif isinstance(node, set):\n",
    "            return list(node)\n",
    "        return node\n",
    "    \n",
    "    return convert_sets_to_lists(hierarchy)\n",
    "\n",
    "def save_hierarchy_to_file(hierarchy: dict, filename: str):\n",
    "    \"\"\"\n",
    "    Save the hierarchy dictionary to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(hierarchy, f, indent=4)\n",
    "\n",
    "def flatten_hierarchy_v1(hierarchy: dict):\n",
    "    \"\"\"\n",
    "    Flatten the hierarchy into a sequential list.\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    \n",
    "    def traverse(node):\n",
    "        if isinstance(node, dict):  # Regular nested dictionary structure\n",
    "            for key, subnode in node.items():\n",
    "                flat_list.append(key)\n",
    "                traverse(subnode)\n",
    "        elif isinstance(node, list):  # Leaf level is a list\n",
    "            for item in node:\n",
    "                flat_list.append(item)\n",
    "\n",
    "    traverse(hierarchy)\n",
    "    return flat_list\n",
    "\n",
    "def flatten_hierarchy(hierarchy: dict):\n",
    "    \"\"\"\n",
    "    Flatten the hierarchy into a sequential list.\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    \n",
    "    def traverse(node):\n",
    "        if isinstance(node, dict):  # Regular nested dictionary structure\n",
    "            for key in node.keys():\n",
    "                flat_list.append(key)\n",
    "            for subnode in node.values():\n",
    "                traverse(subnode)\n",
    "        elif isinstance(node, list):  # Leaf level is a list\n",
    "            for item in node:\n",
    "                flat_list.append(item)\n",
    "\n",
    "    traverse(hierarchy)\n",
    "    return flat_list\n",
    "\n",
    "def define_vocab(df):\n",
    "    hierarchy=build_hierarchy(\n",
    "        df, hierarchy_levels=[\"familyKey\", \"genusKey\", \"speciesKey\"])\n",
    "    vocab=flatten_hierarchy(hierarchy)\n",
    "    return vocab\n",
    "\n",
    "def get_higher_levels(hierarchy: dict, taxa_id, path=None):\n",
    "    \"\"\"\n",
    "    Get the list of all higher levels in the hierarchy for a given taxa_id.\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        path = []\n",
    "    \n",
    "    for key, subnode in hierarchy.items():\n",
    "        new_path = path + [key]\n",
    "\n",
    "        if isinstance(subnode, list):  # If the penultimate level is a list\n",
    "            if taxa_id in subnode:\n",
    "                return new_path + [taxa_id]\n",
    "        elif isinstance(subnode, dict):  # Traverse deeper levels\n",
    "            result = get_higher_levels(subnode, taxa_id, new_path)\n",
    "            if result:\n",
    "                return result\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def invert_hierarchy(hierarchy: dict):\n",
    "    \"\"\"\n",
    "    Generate an inverse tree where each taxa_id points to its higher-level category.\n",
    "    \"\"\"\n",
    "    inverse = {}\n",
    "\n",
    "    def traverse(node, parent=None):\n",
    "        for key, subnode in node.items():\n",
    "            if isinstance(subnode, list):  # If we reach the list level\n",
    "                inverse[key] = parent\n",
    "                for item in subnode:\n",
    "                    inverse[item] = key  # The item belongs to the parent category\n",
    "            elif isinstance(subnode, dict):  # Continue traversing deeper levels\n",
    "                inverse[key] = parent\n",
    "                traverse(subnode, key)\n",
    "\n",
    "    traverse(hierarchy)\n",
    "    return inverse\n",
    "\n",
    "df=pd.read_parquet(parquet_path)\n",
    "df_train_val = df[~df['set'].isin([\"test_ood\"])]\n",
    "hierarchy=build_hierarchy(df_train_val, hierarchy_levels = [\"familyKey\", \"genusKey\", \"speciesKey\"])\n",
    "save_hierarchy_to_file(hierarchy, filename=root_path/\"hierarchy_train.json\")\n",
    "inverse_hierarchy=invert_hierarchy(hierarchy)\n",
    "save_hierarchy_to_file(inverse_hierarchy, filename=root_path/\"inverse_hierarchy_train.json\")\n",
    "vocab=define_vocab(df_train_val)\n",
    "vocab[:10], len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire hierarchy\n",
    "df=pd.read_parquet(parquet_path)\n",
    "hierarchy=build_hierarchy(df, hierarchy_levels = [\"familyKey\", \"genusKey\", \"speciesKey\"])\n",
    "save_hierarchy_to_file(hierarchy, filename=root_path/\"hierarchy_all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_df(df.copy(), remove_in=[\"test_ood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's redefine the dataloader\n",
    "\n",
    "datablock = DataBlock(\n",
    "    blocks=(ImageBlock, MultiCategoryBlock(vocab=vocab)),\n",
    "    splitter=ColSplitter(),\n",
    "    get_x=ColReader(0, pref=images_path),\n",
    "    get_y=ColReader(1, label_delim=' '),\n",
    "    item_tfms=Resize(460),\n",
    "    batch_tfms=aug_transforms(size=224)\n",
    ")\n",
    "dls = datablock.dataloaders(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = F1ScoreMulti(thresh=0.5, average='macro')\n",
    "f1_macro.name = 'F1(macro)'\n",
    "f1_samples = F1ScoreMulti(thresh=0.5, average='samples')\n",
    "f1_samples.name = 'F1(samples)'\n",
    "learn = vision_learner(\n",
    "    dls, \n",
    "    resnet50, \n",
    "    metrics=[partial(accuracy_multi, thresh=0.5), f1_macro, f1_samples],\n",
    "    cbs=[\n",
    "        ShowGraphCallback(),\n",
    "        CSVLogger(export_path/\"history3.csv\"),\n",
    "        EarlyStoppingCallback(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(10, 2e-2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.remove_cbs((CSVLogger,EarlyStoppingCallback))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "model_path = export_path / \"00_lepi_mini_model2\"\n",
    "learn.export(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the size of the exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"/home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1\")\n",
    "\n",
    "learn = load_learner(model_path, cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = Path(\"/home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1-save\")\n",
    "sl = vision_learner(learn.dls.cuda(), resnet50)\n",
    "sl.model = learn.model.cuda()\n",
    "sl.export(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh /home/george/codes/lepinet/data/lepi/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using learner.export, but without the dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "\n",
    "# Estimate memory usage of key attributes\n",
    "print(f\"Total Learner: {asizeof.asizeof(learn.dls.train.items) / 1e6:.2f} MB\")\n",
    "print(f\"Model: {asizeof.asizeof(learn.model) / 1e6:.2f} MB\")\n",
    "print(f\"DataLoaders (dls): {asizeof.asizeof(learn.dls) / 1e6:.2f} MB\" if learn.dls else \"No DataLoaders\")\n",
    "print(f\"Loss function: {asizeof.asizeof(learn.loss_func) / 1e6:.2f} MB\")\n",
    "print(f\"Metrics: {asizeof.asizeof(learn.metrics) / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls = None\n",
    "learn.export(str(model_path)+\"-lw.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using learner.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = Path(\"/home/george/codes/lepinet/data/lepi/models/20250424-lepi-prod_model1-save\")\n",
    "\n",
    "learner.save(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh /home/george/codes/lepinet/data/lepi/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = Path(\"/home/george/codes/lepinet/data/lepi/models/20250424-lepi-prod_model1-save\")\n",
    "\n",
    "learner.load(model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating hierarchy to model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"/home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1-save\")\n",
    "\n",
    "learn = load_learner(model_path, cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy_path=Path(\"/home/george/codes/lepinet/data/lepi/hierarchy_all.json\")\n",
    "\n",
    "with open(hierarchy_path, \"r\") as f:\n",
    "    hierarchy=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.hierarchy = hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = Path(\"/home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1-save-hierarchy\")\n",
    "learn.export(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(model_output_path, cpu=False)\n",
    "print(learn.hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate id2name to model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"/home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1-save-hierarchy\")\n",
    "\n",
    "learn = load_learner(model_path, cpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below: slow, synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_key(scientificName=None, usageKey=None, rank='SPECIES', order='Lepidoptera'):\n",
    "#     \"\"\"Returns taxon key from scientific name.\n",
    "\n",
    "#     Notes\n",
    "#     -----\n",
    "\n",
    "#     Requests GBIF API. \n",
    "\n",
    "#     If GBIF API returns more than one element, display a warning and return the first element from the list.\n",
    "#     \"\"\"\n",
    "\n",
    "#     url = \"https://api.gbif.org/v1/species/match?\"\n",
    "\n",
    "#     assert usageKey is not None or scientificName is not None, \"One of scientificRank or usageKey must be defined.\"\n",
    "\n",
    "#     if usageKey is not None:\n",
    "#         url += f\"usageKey={usageKey}&\"\n",
    "#     if scientificName is not None:\n",
    "#         url += f\"scientificName={scientificName}&\"\n",
    "#     if rank is not None:\n",
    "#         url += f\"rank={rank}&\"\n",
    "#     if order is not None:\n",
    "#         url += f\"order={order}\"\n",
    "\n",
    "#     x=requests.get(url)\n",
    "#     return x.json()\n",
    "\n",
    "# id2name=[get_key(usageKey=k, rank=None) for k in learn.dls.vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below: fast, asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "An asyncio.Future, a coroutine or an awaitable is required",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Run this in an async context\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# If running from a script or Jupyter, use this:\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# import nest_asyncio\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# nest_asyncio.apply()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m id2name = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_all_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/classif/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/classif/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/classif/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/classif/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mget_all_keys\u001b[39m\u001b[34m(vocab)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp.ClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m     20\u001b[39m     tasks = {k:get_key(session, usageKey=k, rank=\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m vocab}\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/classif/lib/python3.12/asyncio/tasks.py:831\u001b[39m, in \u001b[36mgather\u001b[39m\u001b[34m(return_exceptions, *coros_or_futures)\u001b[39m\n\u001b[32m    829\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m coros_or_futures:\n\u001b[32m    830\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m arg_to_fut:\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m         fut = \u001b[43mensure_future\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    832\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    833\u001b[39m             loop = futures._get_loop(fut)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/classif/lib/python3.12/asyncio/tasks.py:689\u001b[39m, in \u001b[36mensure_future\u001b[39m\u001b[34m(coro_or_future, loop)\u001b[39m\n\u001b[32m    687\u001b[39m         should_close = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    688\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mAn asyncio.Future, a coroutine or an awaitable \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    690\u001b[39m                         \u001b[33m'\u001b[39m\u001b[33mis required\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    693\u001b[39m     loop = events.get_event_loop()\n",
      "\u001b[31mTypeError\u001b[39m: An asyncio.Future, a coroutine or an awaitable is required"
     ]
    }
   ],
   "source": [
    "async def get_key(session, scientificName=None, usageKey=None, rank='SPECIES', order='Lepidoptera'):\n",
    "    url = \"https://api.gbif.org/v1/species/match?\"\n",
    "    assert usageKey is not None or scientificName is not None, \"One of scientificName or usageKey must be defined.\"\n",
    "\n",
    "    if usageKey is not None:\n",
    "        url += f\"usageKey={usageKey}&\"\n",
    "    if scientificName is not None:\n",
    "        url += f\"scientificName={scientificName}&\"\n",
    "    if rank is not None:\n",
    "        url += f\"rank={rank}&\"\n",
    "    if order is not None:\n",
    "        url += f\"order={order}\"\n",
    "\n",
    "    async with session.get(url) as response:\n",
    "        r = await response.json()\n",
    "        return r['canonicalName']\n",
    "\n",
    "async def get_all_keys(vocab):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [get_key(session, usageKey=k, rank=None) for k in vocab]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "# Run this in an async context\n",
    "# If running from a script or Jupyter, use this:\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "id2name = asyncio.run(get_all_keys(learn.dls.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.id2name=id2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = Path(\"/home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1-save-hierarchy-id2name\")\n",
    "learn.export(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
