{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "from fastai.vision.all import *\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path=Path(\"/home/george/codes/lepinet/data/mini/0013397-241007104925546_processing_metadata_postprocessed.parquet\")\n",
    "big_parquet_path=Path(\"/home/george/codes/lepinet/data/lepi/0061420-241126133413365_sampled_processing_metadata_postprocessed.parquet\")\n",
    "images_path=Path(\"/home/george/codes/lepinet/data/mini/images\")\n",
    "root_path=Path(\"/home/george/codes/lepinet/data/mini\")\n",
    "export_path=Path(\"/home/george/codes/lepinet/data/mini/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df, remove_in=[], keep_in=[]):\n",
    "    # Filter out 'test_ood' rows and 'test_in' rows\n",
    "    if len(remove_in)>0:\n",
    "        df = df[~df['set'].isin(remove_in)]\n",
    "    if len(keep_in)>0:\n",
    "        df = df[df['set'].isin(keep_in)]\n",
    "    def generate_image_path(row):\n",
    "        return Path(str(row['speciesKey'])) / row['filename']\n",
    "\n",
    "    # Apply the function to create the image paths\n",
    "    df['image_path'] = df.apply(generate_image_path, axis=1)\n",
    "    # Add a column to specify whether the row is for training or validation\n",
    "    df['is_valid'] = df['set'] == '0'\n",
    "    # Define the hierarchical levels\n",
    "    hierarchy_levels = [\"familyKey\", \"genusKey\", \"speciesKey\"]\n",
    "\n",
    "    # Create a function to extract the labels at different hierarchy levels\n",
    "    def get_hierarchy_labels(row):\n",
    "        return ' '.join(map(str, [row[level] for level in hierarchy_levels]))\n",
    "\n",
    "    # Add a column with hierarchy labels\n",
    "    df['hierarchy_labels'] = df.apply(get_hierarchy_labels, axis=1)\n",
    "    # Keep only the columns needed for ImageDataLoaders\n",
    "    df = df[['image_path', 'hierarchy_labels', 'is_valid']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[0]\n",
    "row.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = images_path / row[\"speciesKey\"] / row[\"filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path, os.path.isfile(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(df['set'].isin(['test_ood', '0'])), \n",
    "sum(df['set'].isin(['test_ood'])),\n",
    "sum(df['set'].isin(['0']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=prepare_df(df.copy(), remove_in=['test_ood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_df(\n",
    "    df,\n",
    "    images_path,\n",
    "    valid_col='is_valid',\n",
    "    label_delim=' ',\n",
    "    item_tfms=Resize(460),\n",
    "    batch_tfms=aug_transforms(size=224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = F1ScoreMulti(thresh=0.5, average='macro')\n",
    "f1_macro.name = 'F1(macro)'\n",
    "f1_samples = F1ScoreMulti(thresh=0.5, average='samples')\n",
    "f1_samples.name = 'F1(samples)'\n",
    "learn = vision_learner(\n",
    "    dls, \n",
    "    resnet50, \n",
    "    metrics=[partial(accuracy_multi, thresh=0.5), f1_macro, f1_samples],\n",
    "    cbs=[ShowGraphCallback(), CSVLogger(export_path/\"history1.csv\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(10, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "model_path = export_path / \"00_lepi_mini_model1\"\n",
    "learn.export(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh /home/george/codes/lepinet/data/mini/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = export_path / \"00_lepi_mini_model1\"\n",
    "\n",
    "learn = load_learner(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls.vocab[:10], len(learn.dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model version\n",
    "\n",
    "I need to specify the vocab of the MultiCategoryBlock\n",
    "\n",
    "So I need to go down in the layered architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_vocab_v1(df):\n",
    "    vocab=[]\n",
    "    hierarchy_levels = [\"familyKey\", \"genusKey\", \"speciesKey\"]\n",
    "    for col in hierarchy_levels:\n",
    "        vocab += pd.unique(df[col]).tolist()\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab \n",
    "\n",
    "def build_hierarchy_v1(df: pd.DataFrame, hierarchy_levels: list):\n",
    "    \"\"\"\n",
    "    Build a hierarchical tree where the penultimate level holds a unique list of the lowest-level values.\n",
    "    \"\"\"\n",
    "    hierarchy = defaultdict(lambda: defaultdict(set))  # Use set to avoid duplicates\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        current_level = hierarchy\n",
    "        for i, level in enumerate(hierarchy_levels):\n",
    "            key = row[level]\n",
    "\n",
    "            if i == len(hierarchy_levels) - 2:  # Penultimate level\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = set()\n",
    "                current_level = current_level[key]\n",
    "            elif i == len(hierarchy_levels) - 1:  # Lowest level (store unique values)\n",
    "                current_level.add(key)\n",
    "            else:\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = defaultdict(set)\n",
    "                current_level = current_level[key] # Goes deeper in the hierarchy\n",
    "\n",
    "    # Convert sets to lists for the final output\n",
    "    def convert_sets_to_lists(node):\n",
    "        if isinstance(node, dict):\n",
    "            return {k: convert_sets_to_lists(v) for k, v in node.items()}\n",
    "        elif isinstance(node, set):\n",
    "            return list(node)\n",
    "        return node\n",
    "\n",
    "    return convert_sets_to_lists(hierarchy)\n",
    "\n",
    "def build_hierarchy(df: pd.DataFrame, hierarchy_levels: list):\n",
    "    \"\"\"\n",
    "    Build a hierarchical tree where the penultimate level holds a unique list of the lowest-level values.\n",
    "    \"\"\"\n",
    "    hierarchy = defaultdict(lambda: defaultdict(set))  # Use set to avoid duplicates\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        current_level = hierarchy\n",
    "        for level in hierarchy_levels:\n",
    "            key = row[level]\n",
    "\n",
    "            if level == \"speciesKey\":  # Lowest level (store unique values)\n",
    "                current_level.add(key)\n",
    "            else:\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = set() if level == \"genusKey\" else defaultdict(set)  # Penultimate level\n",
    "                current_level = current_level[key] # Goes deeper in the hierarchy\n",
    "\n",
    "    # Convert sets to lists for the final output\n",
    "    def convert_sets_to_lists(node):\n",
    "        if isinstance(node, dict):\n",
    "            return {k: convert_sets_to_lists(v) for k, v in node.items()}\n",
    "        elif isinstance(node, set):\n",
    "            return list(node)\n",
    "        return node\n",
    "    \n",
    "    return convert_sets_to_lists(hierarchy)\n",
    "\n",
    "def save_hierarchy_to_file(hierarchy: dict, filename: str):\n",
    "    \"\"\"\n",
    "    Save the hierarchy dictionary to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(hierarchy, f, indent=4)\n",
    "\n",
    "def flatten_hierarchy_v1(hierarchy: dict):\n",
    "    \"\"\"\n",
    "    Flatten the hierarchy into a sequential list.\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    \n",
    "    def traverse(node):\n",
    "        if isinstance(node, dict):  # Regular nested dictionary structure\n",
    "            for key, subnode in node.items():\n",
    "                flat_list.append(key)\n",
    "                traverse(subnode)\n",
    "        elif isinstance(node, list):  # Leaf level is a list\n",
    "            for item in node:\n",
    "                flat_list.append(item)\n",
    "\n",
    "    traverse(hierarchy)\n",
    "    return flat_list\n",
    "\n",
    "def flatten_hierarchy(hierarchy: dict):\n",
    "    \"\"\"\n",
    "    Flatten the hierarchy into a sequential list.\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    \n",
    "    def traverse(node):\n",
    "        if isinstance(node, dict):  # Regular nested dictionary structure\n",
    "            for key in node.keys():\n",
    "                flat_list.append(key)\n",
    "            for subnode in node.values():\n",
    "                traverse(subnode)\n",
    "        elif isinstance(node, list):  # Leaf level is a list\n",
    "            for item in node:\n",
    "                flat_list.append(item)\n",
    "\n",
    "    traverse(hierarchy)\n",
    "    return flat_list\n",
    "\n",
    "def define_vocab(df):\n",
    "    hierarchy=build_hierarchy(\n",
    "        df, hierarchy_levels=[\"familyKey\", \"genusKey\", \"speciesKey\"])\n",
    "    vocab=flatten_hierarchy(hierarchy)\n",
    "    return vocab\n",
    "\n",
    "def get_higher_levels(hierarchy: dict, taxa_id, path=None):\n",
    "    \"\"\"\n",
    "    Get the list of all higher levels in the hierarchy for a given taxa_id.\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        path = []\n",
    "    \n",
    "    for key, subnode in hierarchy.items():\n",
    "        new_path = path + [key]\n",
    "\n",
    "        if isinstance(subnode, list):  # If the penultimate level is a list\n",
    "            if taxa_id in subnode:\n",
    "                return new_path + [taxa_id]\n",
    "        elif isinstance(subnode, dict):  # Traverse deeper levels\n",
    "            result = get_higher_levels(subnode, taxa_id, new_path)\n",
    "            if result:\n",
    "                return result\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def invert_hierarchy(hierarchy: dict):\n",
    "    \"\"\"\n",
    "    Generate an inverse tree where each taxa_id points to its higher-level category.\n",
    "    \"\"\"\n",
    "    inverse = {}\n",
    "\n",
    "    def traverse(node, parent=None):\n",
    "        for key, subnode in node.items():\n",
    "            if isinstance(subnode, list):  # If we reach the list level\n",
    "                inverse[key] = parent\n",
    "                for item in subnode:\n",
    "                    inverse[item] = key  # The item belongs to the parent category\n",
    "            elif isinstance(subnode, dict):  # Continue traversing deeper levels\n",
    "                inverse[key] = parent\n",
    "                traverse(subnode, key)\n",
    "\n",
    "    traverse(hierarchy)\n",
    "    return inverse\n",
    "\n",
    "df=pd.read_parquet(parquet_path)\n",
    "df_train_val = df[~df['set'].isin([\"test_ood\"])]\n",
    "hierarchy=build_hierarchy(df_train_val, hierarchy_levels = [\"familyKey\", \"genusKey\", \"speciesKey\"])\n",
    "save_hierarchy_to_file(hierarchy, filename=root_path/\"hierarchy_train.json\")\n",
    "inverse_hierarchy=invert_hierarchy(hierarchy)\n",
    "save_hierarchy_to_file(inverse_hierarchy, filename=root_path/\"inverse_hierarchy_train.json\")\n",
    "vocab=define_vocab(df_train_val)\n",
    "vocab[:10], len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire hierarchy\n",
    "df=pd.read_parquet(parquet_path)\n",
    "hierarchy=build_hierarchy(df, hierarchy_levels = [\"familyKey\", \"genusKey\", \"speciesKey\"])\n",
    "save_hierarchy_to_file(hierarchy, filename=root_path/\"hierarchy_all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_df(df.copy(), remove_in=[\"test_ood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's redefine the dataloader\n",
    "\n",
    "datablock = DataBlock(\n",
    "    blocks=(ImageBlock, MultiCategoryBlock(vocab=vocab)),\n",
    "    splitter=ColSplitter(),\n",
    "    get_x=ColReader(0, pref=images_path),\n",
    "    get_y=ColReader(1, label_delim=' '),\n",
    "    item_tfms=Resize(460),\n",
    "    batch_tfms=aug_transforms(size=224)\n",
    ")\n",
    "dls = datablock.dataloaders(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = F1ScoreMulti(thresh=0.5, average='macro')\n",
    "f1_macro.name = 'F1(macro)'\n",
    "f1_samples = F1ScoreMulti(thresh=0.5, average='samples')\n",
    "f1_samples.name = 'F1(samples)'\n",
    "learn = vision_learner(\n",
    "    dls, \n",
    "    resnet50, \n",
    "    metrics=[partial(accuracy_multi, thresh=0.5), f1_macro, f1_samples],\n",
    "    cbs=[\n",
    "        ShowGraphCallback(),\n",
    "        CSVLogger(export_path/\"history3.csv\"),\n",
    "        EarlyStoppingCallback(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(10, 2e-2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.remove_cbs((CSVLogger,EarlyStoppingCallback))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "model_path = export_path / \"00_lepi_mini_model2\"\n",
    "learn.export(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the size of the exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"/home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1\")\n",
    "\n",
    "learn = load_learner(model_path, cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = Path(\"/home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1-save\")\n",
    "sl = vision_learner(learn.dls.cuda(), resnet50)\n",
    "sl.model = learn.model.cuda()\n",
    "sl.export(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh /home/george/codes/lepinet/data/lepi/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /home/george/codes/lepinet/data/lepi/models/04-lepi-prod_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using learner.export, but without the dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "\n",
    "# Estimate memory usage of key attributes\n",
    "print(f\"Total Learner: {asizeof.asizeof(learn.dls.train.items) / 1e6:.2f} MB\")\n",
    "print(f\"Model: {asizeof.asizeof(learn.model) / 1e6:.2f} MB\")\n",
    "print(f\"DataLoaders (dls): {asizeof.asizeof(learn.dls) / 1e6:.2f} MB\" if learn.dls else \"No DataLoaders\")\n",
    "print(f\"Loss function: {asizeof.asizeof(learn.loss_func) / 1e6:.2f} MB\")\n",
    "print(f\"Metrics: {asizeof.asizeof(learn.metrics) / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls = None\n",
    "learn.export(str(model_path)+\"-lw.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using learner.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = Path(\"/home/george/codes/lepinet/data/lepi/models/20250424-lepi-prod_model1-save\")\n",
    "\n",
    "learner.save(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh /home/george/codes/lepinet/data/lepi/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = Path(\"/home/george/codes/lepinet/data/lepi/models/20250424-lepi-prod_model1-save\")\n",
    "\n",
    "learner.load(model_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
