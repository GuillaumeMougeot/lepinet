{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527a1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from matplotlib_venn import venn2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7dff28",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_path = \"/home/george/codes/lepinet/data/flemming/preds_04-lepi-prod_model1.csv\" \n",
    "lbl_path = \"/home/george/codes/lepinet/data/flemming/normalized.csv\"\n",
    "prd_lbl_path = \"/home/george/codes/lepinet/data/flemming/preds_04-lepi-prod_model1_label.csv\" \n",
    "img_path = \"/home/george/codes/lepinet/data/flemming/images\"\n",
    "name2id_path = \"/home/george/codes/lepinet/data/flemming/name2id.csv\"\n",
    "hier_path = \"/home/george/codes/lepinet/data/lepi/hierarchy_all.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581ba35",
   "metadata": {},
   "source": [
    "Load ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=pd.read_csv(lbl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"genus\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"Orthosia\"\n",
    "rank=\"genus\"\n",
    "# x=requests.get(f\"https://api.gbif.org/v1/species/suggest?higherTaxonKey=797&q={name}&rank={rank.upper()}\")\n",
    "x=requests.get(f\"https://api.gbif.org/v1/species/match?order=Lepidoptera&{rank}={name}\")\n",
    "x.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(name, rank, higherTaxonKey='797'):\n",
    "    \"\"\"Returns taxon key from name and rank.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    Requests GBIF API. \n",
    "\n",
    "    If GBIF API returns more than one element, display a warning and return the first element from the list.\n",
    "    \"\"\"\n",
    "\n",
    "    x=requests.get(f\"https://api.gbif.org/v1/species/suggest?{higherTaxonKey}=797&q={name}&rank={rank.upper()}\")\n",
    "    l=x.json()\n",
    "    if len(l) > 1:\n",
    "        print(f\"Warning: more than one element found in GBIF reply: {l}\")\n",
    "    return l[0]['key']\n",
    "\n",
    "# Find the accepted GBIF keys\n",
    "name2id = {\n",
    "    'verbatimScientificName':[],\n",
    "    'familyKey':[],\n",
    "    'genusKey':[],\n",
    "    'speciesKey':[]\n",
    "}\n",
    "for i, row in lbl.iterrows():\n",
    "    if row['rank'] != 'SPECIES':\n",
    "        print(f\"Warning: wrong rank for row {i} : {row}.\")\n",
    "        continue\n",
    "\n",
    "    speciesKey = row['usageKey'] if pd.isna(row['acceptedUsageKey']) else row['acceptedUsageKey']\n",
    "    genusKey = get_key(row['genus'], 'genus')\n",
    "    familyKey = get_key(row['family'], 'family')\n",
    "    name2id['verbatimScientificName'].append(row['verbatimScientificName'])\n",
    "    name2id['familyKey'].append(familyKey)\n",
    "    name2id['genusKey'].append(genusKey)\n",
    "    name2id['speciesKey'].append(speciesKey)\n",
    "\n",
    "print(name2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1289b7",
   "metadata": {},
   "source": [
    "New attempt by directly using GBIF API fuzzy match system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Tethea or (Denis & Schifferm√ºller), 1776\"\n",
    "x=requests.get(f\"https://api.gbif.org/v1/species/match?order=Lepidoptera&scientificName={name}&strict=True&rank=SPECIES\")\n",
    "x.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece3e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(scientificName=None, usageKey=None, rank='SPECIES', order='Lepidoptera'):\n",
    "    \"\"\"Returns taxon key from scientific name.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    Requests GBIF API. \n",
    "\n",
    "    If GBIF API returns more than one element, display a warning and return the first element from the list.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://api.gbif.org/v1/species/match?\"\n",
    "\n",
    "    assert usageKey is not None or scientificName is not None, \"One of scientificRank or usageKey must be defined.\"\n",
    "\n",
    "    if usageKey is not None:\n",
    "        url += f\"usageKey={usageKey}&\"\n",
    "    if scientificName is not None:\n",
    "        url += f\"scientificName={scientificName}&\"\n",
    "    if rank is not None:\n",
    "        url += f\"rank={rank}&\"\n",
    "    if order is not None:\n",
    "        url += f\"order={order}\"\n",
    "\n",
    "    x=requests.get(url)\n",
    "    return x.json()\n",
    "\n",
    "# list folder names\n",
    "foldernames = os.listdir(img_path)\n",
    "\n",
    "name2id = {\n",
    "    'verbatimScientificName':[],\n",
    "    'familyKey':[],\n",
    "    'genusKey':[],\n",
    "    'speciesKey':[]\n",
    "}\n",
    "\n",
    "for i, f in enumerate(foldernames):\n",
    "    k=get_key(scientificName=f)\n",
    "    if k['rank']!='SPECIES':\n",
    "        print(f\"Wrong rank for {f} : {k}\")\n",
    "    if f == 'Tethea or': # Bug fix with GBIF Species API, Tethea or gives the order\n",
    "        k=get_key(usageKey=\"5142971\")\n",
    "    name2id['verbatimScientificName'].append(f)\n",
    "    name2id['familyKey'].append(k['familyKey'])\n",
    "    name2id['genusKey'].append(k['genusKey'])\n",
    "    speciesKey = k['usageKey'] if 'acceptedUsageKey' not in k.keys() else k['acceptedUsageKey']\n",
    "    name2id['speciesKey'].append(speciesKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(name2id)\n",
    "df.to_csv(name2id_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bece81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250340b",
   "metadata": {},
   "source": [
    "Let's make some Venn diagram with model vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549860f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hier_path, \"r\") as f:\n",
    "    hier=json.load(f)\n",
    "model_families = list(hier.keys())\n",
    "model_genuses = []\n",
    "model_species = []\n",
    "for fk, fv in hier.items():\n",
    "    for gk, gv in fv.items():\n",
    "        model_genuses.append(gk)\n",
    "        for s in gv:\n",
    "            model_species.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_families), len(model_genuses), len(model_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b74930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_species[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fe0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name2id['speciesKey'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6348d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_species = set([int(f) for f in model_species])\n",
    "target_species = set(name2id['speciesKey'])\n",
    "venn2((model_species, target_species), set_labels=(\"Model vocab\", \"Target vocab\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_genuses = set([int(f) for f in model_genuses])\n",
    "target_genuses = set(name2id['genusKey'])\n",
    "venn2((model_genuses, target_genuses), set_labels=(\"Model vocab\", \"Target vocab\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5de1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "forgotten_species=target_species-model_species\n",
    "\n",
    "def gbif_match(id):\n",
    "    x=requests.get(f\"https://api.gbif.org/v1/species/match?usageKey={id}\")\n",
    "    return x.json()\n",
    "\n",
    "[gbif_match(f) for f in forgotten_species]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d571d5",
   "metadata": {},
   "source": [
    "Get the model predictions\n",
    "\n",
    "A model prediction is a tuple (level, prediction, confidence).\n",
    "\n",
    "The level is one of (None, species, genus, family)\n",
    "\n",
    "None means that the model did not find anything in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd=pd.read_csv(prd_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd[:6].to_csv(\"/home/george/codes/lepinet/data/flemming/preds_04-lepi-prod_model1-sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c68f6a",
   "metadata": {},
   "source": [
    "Let's add the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ec9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename2taxa(filename: str, df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Example: \n",
    "    In: 'flemming/images/Orthosia incerta/crop_TRAPNAME.jpg', name2id DataFrame\n",
    "    Out: {'verbatimScientificName': {0: 'Orthosia incerta'}, 'familyKey': {0: 7015}, 'genusKey': {0: 1798902}, 'speciesKey': {0: 1799135}}\n",
    "    \"\"\"\n",
    "    f = Path(filename).parent.name\n",
    "    return df[df['verbatimScientificName']==f].iloc[0].to_dict()\n",
    "\n",
    "lbls = []\n",
    "\n",
    "level2rank = {\n",
    "    0:'speciesKey',\n",
    "    1:'genusKey',\n",
    "    2:'familyKey'\n",
    "}\n",
    "\n",
    "for i, row in prd.iterrows():\n",
    "    taxa=filename2taxa(row['filename'], df=df)\n",
    "    lbls.append(taxa[level2rank[row['level']]])\n",
    "\n",
    "lbls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f53a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd['label']=lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd[['level','prediction','confidence','label']].tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prd.to_csv(prd_lbl_path, index=False)\n",
    "prd = pd.read_csv(prd_lbl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c076641",
   "metadata": {},
   "source": [
    "Let's get some metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(df):\n",
    "    # Add an instance ID for grouping every 3 rows as one instance\n",
    "    df['instance_id'] = df.index // 3\n",
    "\n",
    "    # Set the confidence threshold\n",
    "    threshold = 0.5\n",
    "\n",
    "    correct_count = 0\n",
    "    total_instances = df['instance_id'].nunique()\n",
    "\n",
    "    # Group by each instance\n",
    "    for instance_id, group in df.groupby('instance_id'):\n",
    "        group_sorted = group.sort_values('level')  # Ensure levels are sorted from 0 to 2\n",
    "        for _, row in group_sorted.iterrows():\n",
    "            if row['confidence'] >= threshold:\n",
    "                if row['prediction'] == row['label']:\n",
    "                    correct_count += 1\n",
    "                break  # Only consider the *lowest* confident level\n",
    "        # If no level passed threshold ‚Üí no increment (considered incorrect)\n",
    "\n",
    "    accuracy = correct_count / total_instances\n",
    "    print(f\"Accuracy at confidence threshold {threshold}: {accuracy:.2%}\")\n",
    "    return accuracy\n",
    "acc = get_accuracy(prd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a087b9e",
   "metadata": {},
   "source": [
    "üîπ 1. Coverage\n",
    "\n",
    "Definition: Proportion of instances where the model made any prediction (i.e., had confidence ‚â• threshold at some level).\n",
    "\n",
    "Rationale: Measures how often the model is confident enough to make a prediction.\n",
    "\n",
    "Formula:\n",
    "\n",
    "coverage = num_instances_with_prediction / total_instances\n",
    "\n",
    "üîπ 2. Correct @ Level\n",
    "\n",
    "Definition: Accuracy at each level (0, 1, 2), conditional on that level being the one used for the prediction.\n",
    "\n",
    "Rationale: Helps diagnose at which levels the model tends to be more or less reliable.\n",
    "\n",
    "üîπ 3. Average Prediction Level\n",
    "\n",
    "Definition: The average level (0 being lowest) at which a prediction was made.\n",
    "\n",
    "Rationale: Reflects how \"deep\" the model usually goes before being confident enough ‚Äî useful for understanding how fine-grained predictions are.\n",
    "\n",
    "Lower is better (if lower levels are more specific and desirable).\n",
    "\n",
    "üîπ 4. Threshold-based Precision/Recall (Hierarchical Precision/Recall)\n",
    "\n",
    "For multi-level classification, define:\n",
    "\n",
    "Hierarchical Precision: Did the model predict the correct label at any level above the threshold?\n",
    "\n",
    "Hierarchical Recall: Out of all levels, how many did it get right when confidence allowed a prediction?\n",
    "\n",
    "Can be defined flexibly based on your use case (e.g., allowing partial credit for parent-level matches).\n",
    "\n",
    "üîπ 5. No-Prediction Rate\n",
    "\n",
    "Definition: Proportion of instances where no prediction was made due to all confidence values being below the threshold.\n",
    "\n",
    "Helps quantify abstention behavior.\n",
    "\n",
    "üîπ 6. Mean Confidence of Correct vs Incorrect Predictions\n",
    "\n",
    "Helps to see whether confidence is calibrated ‚Äî i.e., are confident predictions actually more likely to be correct?\n",
    "\n",
    "üîπ 7. Hierarchical Distance (Optional)\n",
    "\n",
    "If you have a tree/graph structure of the labels, compute the distance between predicted and true labels in the hierarchy.\n",
    "\n",
    "This allows \"almost correct\" predictions to be graded more gracefully than flat accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Core evaluation function to identify predictions above threshold per instance\n",
    "def evaluate_predictions(df, threshold=0.5, merge=True):\n",
    "    \"\"\"\n",
    "    Returns a summary dataframe with one row per instance:\n",
    "    - prediction_level: level where prediction was made (lowest with confidence >= threshold)\n",
    "    - correct: whether the prediction was correct\n",
    "    - prediction_made: whether any prediction was made\n",
    "    \"\"\"\n",
    "    if not 'instance_id' in df.keys():\n",
    "        df['instance_id'] = df.index // 3\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for instance_id, group in df.groupby('instance_id'):\n",
    "        group_sorted = group.sort_values('level')\n",
    "        pred_made = False\n",
    "        for _, row in group_sorted.iterrows():\n",
    "            if row['confidence'] >= threshold:\n",
    "                pred_made = True\n",
    "                summary.append({\n",
    "                    'instance_id': instance_id,\n",
    "                    'prediction_level': row['level'],\n",
    "                    'correct': int(row['prediction'] == row['label']),\n",
    "                    'prediction_made': 1\n",
    "                })\n",
    "                break\n",
    "        if not pred_made:\n",
    "            summary.append({\n",
    "                'instance_id': instance_id,\n",
    "                'prediction_level': np.nan,\n",
    "                'correct': 0,\n",
    "                'prediction_made': 0\n",
    "            })\n",
    "\n",
    "    summary = pd.DataFrame(summary)\n",
    "    if merge:\n",
    "        summary = df.merge(summary, on='instance_id')\n",
    "    return summary\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(summary_df):\n",
    "    return summary_df['correct'].mean()\n",
    "\n",
    "# Coverage\n",
    "def coverage(summary_df):\n",
    "    return summary_df['prediction_made'].mean()\n",
    "\n",
    "# Coverage per level\n",
    "def coverage_per_level(summary_df):\n",
    "    return summary_df['prediction_level'].value_counts(dropna=False).sort_index()/len(summary_df)\n",
    "\n",
    "# Correct @ Level\n",
    "def correct_at_each_level(summary_df):\n",
    "    level_accuracy = summary_df.dropna().groupby('prediction_level')['correct'].mean()\n",
    "    return level_accuracy.to_dict()\n",
    "\n",
    "# Average Prediction Level\n",
    "def average_prediction_level(summary_df):\n",
    "    return summary_df['prediction_level'].mean()\n",
    "\n",
    "# No Prediction Rate\n",
    "def no_prediction_rate(summary_df):\n",
    "    return 1 - coverage(summary_df)\n",
    "\n",
    "# Mean Confidence of Correct vs Incorrect Predictions\n",
    "def confidence_stats(summary_df):\n",
    "    result = {}\n",
    "    for outcome in [0, 1]:\n",
    "        ids = summary_df.loc[summary_df['correct'] == outcome, 'instance_id']\n",
    "        subset = summary_df[summary_df['instance_id'].isin(ids)]\n",
    "        result[f'mean_confidence_correct_{outcome}'] = subset['confidence'].mean()\n",
    "    return result\n",
    "\n",
    "# Run all metrics in one call\n",
    "def evaluate_all_metrics(summary):\n",
    "    return {\n",
    "        'accuracy': accuracy(summary),\n",
    "        'coverage': coverage(summary),\n",
    "        'coverage_per_level' : coverage_per_level(summary),\n",
    "        'average_prediction_level': average_prediction_level(summary),\n",
    "        'correct_at_each_level': correct_at_each_level(summary),\n",
    "        **confidence_stats(summary)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = evaluate_predictions(prd, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ed1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[:6].to_csv(\"/home/george/codes/lepinet/data/flemming/preds_04-lepi-prod_model1_label-sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e61b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all_metrics(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15db4f9",
   "metadata": {},
   "source": [
    "Show some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_examples(summary_df, n=5):\n",
    "    def get_instance_ids(filter_fn):\n",
    "        subset = summary_df[filter_fn(summary_df)]\n",
    "        return subset.sample(n=min(n, len(subset)))['instance_id'].values\n",
    "\n",
    "    categories = [\n",
    "        (\"‚úÖ Good prediction (correct at level 0)\", lambda s: (s['correct'] == 1) & (s['prediction_level'] == 0)),\n",
    "        (\"‚ùå Incorrect prediction despite high confidence (> 0.9)\", lambda s: (s['correct'] == 0) & (s['prediction_level'] == 0) & (s['confidence'] > 0.9)),\n",
    "        (\"ü§î Model abstained (no confidence ‚â• threshold)\", lambda s: (s['prediction_made'] == 0)),\n",
    "    ]\n",
    "\n",
    "    for title, filter_fn in categories:\n",
    "        instance_ids = get_instance_ids(filter_fn)\n",
    "        if len(instance_ids) == 0:\n",
    "            print(f\"\\n--- {title} ---\\nNo examples found.\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- {title} ({len(instance_ids)} example(s)) ---\")\n",
    "        fig, axes = plt.subplots(1, len(instance_ids), figsize=(4 * len(instance_ids), 4))\n",
    "        if len(instance_ids) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for ax, instance_id in zip(axes, instance_ids):\n",
    "            instance_rows = summary_df[summary_df['instance_id'] == instance_id].sort_values('level')\n",
    "            img_path = instance_rows.iloc[0]['filename']\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Image not found: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            img = Image.open(img_path)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            pred_level = instance_rows.iloc[0]['prediction_level']\n",
    "            correct = instance_rows.iloc[0]['correct']\n",
    "            pred = instance_rows.iloc[0]['prediction']\n",
    "            label = instance_rows.iloc[0]['label']\n",
    "            ax.set_title(f\"Level: {pred_level}\\nCorrect: {bool(correct)}\\nPrediction: {pred}\\nLabel: {label}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_examples(summary, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd94f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction_and_samples(input_image_path, predicted_class, true_class, dataset_root, n_samples=6):\n",
    "    \"\"\"\n",
    "    Displays:\n",
    "    1. The input image\n",
    "    2. n_samples images from the predicted class\n",
    "    3. n_samples images from the true class\n",
    "\n",
    "    Parameters:\n",
    "    - input_image_path (str): Path to the input image.\n",
    "    - predicted_class (str): Predicted class name.\n",
    "    - true_class (str): Ground truth class name.\n",
    "    - dataset_root (str): Root directory containing class subfolders.\n",
    "    - n_samples (int): Number of sample images to show for each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_sample_images(class_name):\n",
    "        class_dir = os.path.join(dataset_root, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            raise ValueError(f\"Class directory '{class_dir}' not found.\")\n",
    "        image_files = [f for f in os.listdir(class_dir)\n",
    "                       if os.path.isfile(os.path.join(class_dir, f)) and\n",
    "                       f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff'))]\n",
    "        if not image_files:\n",
    "            raise ValueError(f\"No image files found in class directory '{class_dir}'.\")\n",
    "        selected = random.choices(image_files, k=n_samples) if len(image_files) < n_samples else random.sample(image_files, n_samples)\n",
    "        return [os.path.join(class_dir, f) for f in selected]\n",
    "\n",
    "    def plot_images(image_paths, titles, title_prefix):\n",
    "        total = len(image_paths)\n",
    "        cols = min(total, 5)\n",
    "        rows = math.ceil(total / cols)\n",
    "        plt.figure(figsize=(3.5 * cols, 3.5 * rows))\n",
    "        for i, (img_path, title) in enumerate(zip(image_paths, titles)):\n",
    "            img = Image.open(img_path)\n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"{title_prefix}: {title}\", fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 1. Input image\n",
    "    plot_images([input_image_path], [os.path.basename(os.path.dirname(input_image_path))], \"Input\")\n",
    "\n",
    "    # 2. Predicted class samples\n",
    "    pred_images = get_sample_images(predicted_class)\n",
    "    plot_images(pred_images, [predicted_class] * n_samples, \"Predicted\")\n",
    "\n",
    "    # 3. True class samples\n",
    "    true_images = get_sample_images(true_class)\n",
    "    plot_images(true_images, [true_class] * n_samples, \"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_fn = lambda s: (s['correct'] == 0) & (s['prediction_level'] == 0) & (s['confidence'] > 0.7) & (s['level'] == 0)\n",
    "subset = summary[filter_fn(summary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = subset.sample(n=1).iloc[0]\n",
    "\n",
    "print(row)\n",
    "\n",
    "display_prediction_and_samples(\n",
    "    input_image_path=row['filename'],\n",
    "    predicted_class=str(row['prediction']),\n",
    "    true_class=str(row['label']),\n",
    "    dataset_root=\"/home/george/codes/lepinet/data/lepi/images\",\n",
    "    n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea2598",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ab2bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_path = Path(\"C:\\\\Users\\\\au761367\\\\OneDrive - Aarhus universitet\\\\Codes\\\\python\\\\lepinet\\\\data\\\\preds\\\\flemming_resized.csv\" )\n",
    "# lbl_path = \"/home/george/codes/lepinet/data/flemming/normalized.csv\"\n",
    "# prd_lbl_path = \"/home/george/codes/lepinet/data/flemming/preds_04-lepi-prod_model1_label.csv\" \n",
    "# img_path = \"/home/george/codes/lepinet/data/flemming/images\"\n",
    "# name2id_path = \"/home/george/codes/lepinet/data/flemming/name2id.csv\"\n",
    "# hier_path = \"/home/george/codes/lepinet/data/lepi/hierarchy_all.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read df\n",
    "df=pd.read_csv(prd_path)\n",
    "\n",
    "# add instance_id column\n",
    "df['instance_id'] = np.repeat(np.arange(len(df)//3),3)\n",
    "\n",
    "# invert level 0 and level 2\n",
    "df['level'] = df['level'].replace({0: 2, 2: 0})\n",
    "\n",
    "# add threshold column\n",
    "df['threshold'] = 0.5\n",
    "\n",
    "# add label column\n",
    "df['label'] = df['filename'].apply(lambda x: Path(x).parts[-2])\n",
    "\n",
    "# save new df\n",
    "df.to_csv(prd_path.with_stem(prd_path.stem+\"_reordered\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
